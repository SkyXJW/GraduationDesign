
from datasets import load_dataset
import re
import json
from tqdm import tqdm
import time

from dataset.APPSHandler import *
from prompt.template import *
from utils.utils import *

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"
from transformers import AutoTokenizer, AutoModelForCausalLM

RESULTS_FILE = "/home/xjg/GraduationDesign/results/Qwen_experiments_tot.json"

class Node:
    def __init__(self, state, score=0.0):
        self.state = state  # å½“å‰é—®é¢˜æè¿°(è¿˜å¯èƒ½åŒ…å«ä»£ç ç”Ÿæˆæ€è·¯)
        self.score = score

class ToT:
    def __init__(self, tokenizer, model, k=2, b=1, T=2, public_tests=[], private_tests=[]):
        self.tokenizer = tokenizer
        self.model = model
        self.k = k # è®¾ç½®æ¯ä¸ªèŠ‚ç‚¹çš„æ‰©å±•å­èŠ‚ç‚¹æ•°ç›®ä¸º2
        self.b = b # è¿™é‡Œçš„TOTé‡‡ç”¨BFSæœç´¢ç­–ç•¥ï¼Œè®¾ç½®æœç´¢å®½åº¦ä¸º1ï¼Œå³æ˜¯è¯´æ¯ä¸€æ­¥éƒ½ä»…ä¿å­˜ä¸€ä¸ªå€™é€‰é¡¹
        self.T = T # è®¾ç½®æœç´¢æ€»æ­¥æ•°ä¸º2
        self.public_tests = public_tests
        self.private_tests = private_tests
    
    # è¿”å›[thought-1...thought-k]æ ¼å¼çš„thoughtæ•°ç»„åˆ—è¡¨
    def thought_generator(self, node):
        if not node["thought"]: # è¿™é‡Œè¯´æ˜å½“å‰æ‰©å±•çš„nodeæ˜¯root,æ­¤æ—¶ä»…æœ‰problem
            inputs = self.tokenizer(tot_thought_generation_no_thought.format(node["problem"],self.k), return_tensors="pt")
            # print(tot_thought_generation_no_thought.format(node["problem"],self.k))
        else:
            inputs = tokenizer(tot_thought_generation_with_thought.format(self.k,node["problem"],node["thought"]), return_tensors="pt")
            # print(tot_thought_generation_with_thought.format(self.k,node["problem"],node["thought"]))

        outputs = self.model.generate(input_ids=inputs.input_ids.cuda(), 
                    attention_mask=inputs.attention_mask.cuda(),
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    max_new_tokens=100000, 
                    num_return_sequences=1)
        new_thoughts = [tokenizer.decode(output[inputs.input_ids.size(1):].cpu(),
                         skip_special_tokens=True).strip() for output in outputs]
        # print(new_thoughts[0])
    
        json_str = extract_json(new_thoughts[0])
        # print(manual_json_parse(json_str))
        return manual_json_parse(json_str)


    def thought_evaluator(self, node):
        # print(thought_evaluation.format(node["problem"],node["thought"]))
        inputs = tokenizer(thought_evaluation.format(node["problem"],node["thought"]),return_tensors="pt")
        outputs = model.generate(input_ids=inputs.input_ids.cuda(), 
                            attention_mask=inputs.attention_mask.cuda(),
                            pad_token_id=tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id,
                            max_new_tokens=100000,
                            num_return_sequences=1) # è®¾ç½®è¾“å‡ºå›ç­”æ•°ç›®ä¸º3æ¡
        response = [tokenizer.decode(output[inputs.input_ids.size(1):].cpu(),
                        skip_special_tokens=True).strip() for output in outputs]
        # print(response[0])
        # è¡¥ä¸ï¼š Qwenç”Ÿæˆçš„responseæ ¼å¼è¿˜å¯èƒ½ä¸ºxxxx. The correctness score is x.
        match = re.search(r"evaluation:\s*(-?\d+(?:\.\d+)?)| The correctness score is\s*(-?\d+(?:\.\d+)?)", response[0])
        if match:
            return float(match.group(1) if match.group(1) else match.group(2))
        else:
            raise ValueError("Failure to capture valid evaluation scores")
    
    def TOT_Thought_BFS(self, node):
        S = []
        S.append(node) # åˆå§‹èŠ‚ç‚¹nodeä»…åŒ…å«é—®é¢˜æè¿°
        for _ in range(self.T):
            S_Temp = []
            for s in S:
                Z = self.thought_generator(s)
                for z in Z:
                    new_node = Node({"problem": s.state["problem"], "thought": z["Thought"]}, score)
                    score = self.thought_evaluator(new_node)
                    new_node.score = score
                    S_Temp.append(new_node)
            S_Temp.sort(key=lambda node: node.score, reverse=True)
            S = S_Temp[:self.b]
        # æœ€ç»ˆæŒ‰ç…§æœç´¢è¿‡ç¨‹ä¸­å¾—åˆ°çš„å¾—åˆ†æœ€é«˜çš„thoughtï¼Œç”Ÿæˆåˆä¸€æ–°thoughtï¼Œå¹¶å°†å…¶ä½œä¸ºæœ€ç»ˆç»“æœè¿”å›
        Z = self.thought_generator(S[0])
        return Z[0]


def main():
    # åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained("/home/xjg/checkpoints/Qwen2.5-32B", trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained("/home/xjg/checkpoints/Qwen2.5-32B", device_map="auto", trust_remote_code=True)
    # åˆå§‹åŒ–TOT
    tot = TOT(tokenizer, model)
    test_data = load_proceed(100) # å„ä¸ªéš¾åº¦çš„é¢˜ç›®æŠ½100é“
    first_write = False

    for category, dataset in test_data.items():
        print(f"ğŸ“‚ å¤„ç†ç±»åˆ«: {category}ï¼Œå…± {len(dataset)} é“é¢˜ç›®")

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        for row in tqdm(dataset, desc=f"Processing {category}"):
            if category != "introductory" or row["id"] > 4029:
                continue
            # res = run_tests(code, row["private_tests"])
            # print("here")
            # print(res)
            # # è®°å½•ç»“æœ
            # result_entry = {
            #     "category": category,
            #     "question_id": row['id'],
            #     "time_taken": "Timeout",
            #     "pass_rate": res["pass_rate"],
            #     "thought": "",
            #     "best_code": code  # ä¹Ÿå¯ä»¥å­˜å‚¨ä»£ç 
            # }
            # save_result_entry(result_entry, is_first=first_write)
            # exit()
            # if category == "introductory" and row["id"] > 4029:
            #     continue
            # if category == "interview" and row["id"] > 29:
            #     continue
            # if category == "competition" and row["id"] > 3029:
            #     continue
            question_id = row["id"]
            tot.public_tests = row["public_tests"]
            tot.private_tests = row["private_tests"]

            root = Node({"problem": row["question"], "thought": ""})

            # æŒ‰ç…§BFSå¯¹TOTè¿›è¡Œæœç´¢
            best_thought = tot.TOT_Thought_BFS(root)
            _ , best_code = generate_code(tokenizer, model, row["question"], best_thought)

            # è¯„ä¼°ç§æœ‰æµ‹è¯•é›†
            private_result = run_tests(best_code, row["private_tests"])

            # è®°å½•ç»“æœ
            result_entry = {
                "category": category,
                "question_id": question_id,
                "pass_rate": private_result["pass_rate"],
                "thought": best_thought,
                "best_code": best_code
            }
            save_result_entry(result_entry, RESULTS_FILE, is_first=first_write)
            first_write = False

            print(f"âœ… {category} - {question_id} å¤„ç†å®Œæˆï¼Œç§æœ‰æµ‹è¯•é€šè¿‡ç‡: {private_result['pass_rate']:.2f}")
    finalize_results(RESULTS_FILE)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³", RESULTS_FILE)
if __name__ == '__main__':
    main()
